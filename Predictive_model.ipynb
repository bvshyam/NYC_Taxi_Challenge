{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Challenge - Model Creation\n",
    "By Shyam Balagurumurthy Viswanathan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Build a predictive model for tip as a percentage of the total fare\n",
    "\n",
    "As part of the challenge, we will use data collected by the New York City Taxi and Limousine commission about \"Green\" Taxis. We are using NYC Taxi and Limousine trip record data: (http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml). We will build a model to pre\n",
    "\n",
    "We will build a model to predict the 'Tip_percentage' provided on each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import dill\n",
    "import os\n",
    "import geopy.distance as geo\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_distance(x):\n",
    "    \"\"\"Return great circle distance of pickup and dropoff point\"\"\"\n",
    "    return(geo.great_circle((x.Pickup_latitude,x.Pickup_longitude),(x.Dropoff_latitude,x.Dropoff_longitude)).miles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataPrep(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def fit(self, X, y =None):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Perform Feature Engineering and handle NAN\"\"\"\n",
    "        # Drop the column as it is NA\n",
    "        X = pd.DataFrame(X.drop('Ehail_fee',axis=1))\n",
    "        \n",
    "        X['dates'] = pd.to_datetime(X.lpep_pickup_datetime)\n",
    "        # Day of ride\n",
    "        X['day'] = X['dates'].dt.day\n",
    "        # Hour of ride\n",
    "        X['hour'] = X['dates'].dt.hour\n",
    "        # Day of week ride\n",
    "        X['dayofweek'] = X['dates'].dt.dayofweek\n",
    "        \n",
    "        # Convet to categorical variables and convert missing values to NA\n",
    "        X.VendorID = X.VendorID.astype('category',categories=[1, 2],ordered=False)\n",
    "        X.RateCodeID = X.RateCodeID.astype('category',categories=[1, 2, 3, 4, 5, 6],ordered=False)\n",
    "        X.Store_and_fwd_flag = X.Store_and_fwd_flag.astype('category',categories=['Y','N'],ordered=False)\n",
    "        X.Payment_type = X.Payment_type.astype('category',categories=[1, 2, 3, 4, 5, 6],ordered=False)\n",
    "        X.Trip_type = X.Trip_type.astype('category',categories=[1, 2],ordered=False)\n",
    "                       \n",
    "        # Drop uncessary columns \n",
    "        X.drop(configstore['drop_cols'],axis=1,inplace=True)\n",
    "     \n",
    "        #print(X.info())\n",
    "        print(\"Data preparation completed successfully\")\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Perform Imputation for categorical variables\"\"\"\n",
    "\n",
    "        self.model = Imputer(strategy='median')\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        #Fit the model on categorical missing cols\n",
    "        self.model.fit(X[configstore['categorical_cols']])\n",
    "        return self \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        #Transform the model and convert it to dummies\n",
    "        X[configstore['categorical_cols']]= self.model.transform(X[configstore['categorical_cols']])\n",
    "        X_new = pd.get_dummies(X,drop_first=True,prefix_sep='_',columns=['VendorID','Store_and_fwd_flag','RateCodeID','Payment_type','Trip_type'])\n",
    "        \n",
    "        #print(X_new.info())\n",
    "        print(\"Data Imputation on categorical variables completed successfully\")\n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the year(4 digits): 2015\n",
      "Enter the month number: 9\n",
      "Dataset and Config loaded successfully\n",
      "Train/Test split performed successfully\n",
      "Tip percentage calculated\n",
      "No records more than threshold miles\n",
      "Data preparation completed successfully\n",
      "Data Imputation on categorical variables completed successfully\n",
      "Data preparation completed successfully\n",
      "Data Imputation on categorical variables completed successfully\n",
      "Model output file stored as Pickle!\n",
      "Mean Squared Error: 0.01212023305439051\n",
      "R-squared: 0.9998478634304127\n"
     ]
    }
   ],
   "source": [
    "# Main function which loads the datasets and calls other functions\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    #Get year, month \n",
    "    year = str(input(\"Enter the year(4 digits): \")).zfill(4)\n",
    "    month = str(input(\"Enter the month number: \")).zfill(2)\n",
    "    \n",
    "    url = 'https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_'+year+'-'+month+'.csv'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(url,sep=',')\n",
    "        \n",
    "    except:\n",
    "        print(\"Error in inputs. Downloading default dataset of year 2015 and September month.\")\n",
    "        df = pd.read_csv('https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2015-09.csv',sep=',')\n",
    "    \n",
    "    with open('./config/config.json','r') as f:\n",
    "        configstore =  json.load(f)\n",
    "    \n",
    "    print(\"Dataset and Config loaded successfully\")\n",
    "    \n",
    "    try:\n",
    "        df.columns = configstore['all_cols']\n",
    "\n",
    "        tip =((df['Tip_amount']/df['Total_amount']).round(2))*100\n",
    "        tip[tip.isna()]=0\n",
    "\n",
    "        df['Tip_percent']= tip\n",
    "\n",
    "        # Split the data between Train and test dataset. Have given a test size of 20%. Random split, or datewise split\n",
    "\n",
    "        if ast.literal_eval(configstore['normal_split']):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df.drop(['Tip_percent'], axis=1), df['Tip_percent'],test_size=configstore['split_size'], random_state=40)\n",
    "        else:\n",
    "            # Split around 20% of data\n",
    "            n = df.shape[0]\n",
    "            train_size= configstore['split_size']\n",
    "            X_train, X_test, y_train, y_test = df.drop(['Tip_percent'], axis=1).iloc[:int(n*train_size)] , df.drop(['Tip_percent'], axis=1).iloc[int(n*train_size):], \\\n",
    "                                            df['Tip_percent'][:int(n*train_size)], df['Tip_percent'][int(n*train_size):]\n",
    "\n",
    "\n",
    "        print(\"Train/Test split performed successfully\")\n",
    "\n",
    "        X_train['Tip_percent'] = y_train    \n",
    "        print(\"Tip percentage calculated\")\n",
    "\n",
    "\n",
    "        if ast.literal_eval(configstore['less_totals']):\n",
    "            #Totals less than 0 are removed. Tip % amount cant be calculated\n",
    "            X_train = X_train[X_train.Total_amount >0]\n",
    "\n",
    "        if ast.literal_eval(configstore['less_distance']):\n",
    "            #Distance less than or equal to 0 are removed. Need valid distance for a trip\n",
    "            X_train = X_train[X_train.Trip_distance > 0]\n",
    "\n",
    "        if ast.literal_eval(configstore['outliers']):\n",
    "            # Distance calculated from latitude/long for trip distance equal to 0\n",
    "            new_trip_distance = pd.Series(round(X_train[np.logical_and(X_train.Trip_distance == 0,~ np.logical_or(X_train.Pickup_longitude == 0,X_train.Dropoff_longitude == 0))]\\\n",
    "                .apply(calculate_distance,axis=1),2),name='Trip_distance')\n",
    "            X_train.update(new_trip_distance)\n",
    "\n",
    "        if ast.literal_eval(configstore['correct_distance']):\n",
    "            #Removing the outliers which are wrong data for this dataset\n",
    "            try:\n",
    "                X_train.drop([X_train[X_train['Trip_distance'] >= mean + 100*sd].index.values[0]],inplace=True)\n",
    "                print('Records more than thresold miles are dropped')\n",
    "            except:\n",
    "                print('No records more than threshold miles')\n",
    "\n",
    "\n",
    "        # Perfrom split after cleaning the Training dataset\n",
    "        y_train = X_train['Tip_percent']\n",
    "        X_train.drop(['Tip_percent'],axis=1,inplace=True)\n",
    "\n",
    "        # Steps to be performed for pipeline\n",
    "        steps = [('clean_dataset',DataPrep()),('imputing',DataFrameImputer()),\\\n",
    "                 ('scaler',StandardScaler()),#()\n",
    "                ('regr',RandomForestRegressor())]\n",
    "\n",
    "        # Grid search parameters\n",
    "        parameters = [{'regr':[RandomForestRegressor()], 'regr__n_estimators':[30],'regr__random_state':[40]}]\n",
    "\n",
    "        # Sklearn pipeline\n",
    "        pl = Pipeline(steps)\n",
    "\n",
    "        # Gridsearch CV on the dataset not used due to memory limitations\n",
    "        #model = GridSearchCV(pl,parameters,cv=configstore['CV'],n_jobs=-1)\n",
    "        #model.fit(X_train,y_train)\n",
    "\n",
    "        # Fitting the model using pipeline     \n",
    "        pl.fit(X_train,y_train)\n",
    "\n",
    "        # Performing prediction using the pipeline\n",
    "        y_pred = pl.predict(X_test)\n",
    "\n",
    "        #Store the model for future usage\n",
    "        if not os.path.exists('./generated_files'):\n",
    "            os.makedirs('./generated_files')\n",
    "\n",
    "        with open(\"./generated_files/model.pkl\",'wb') as model_file:\n",
    "            dill.dump(pl,model_file)\n",
    "        print(\"Model output file stored as Pickle!\")\n",
    "        print(\"Mean Squared Error: {}\".format(mean_squared_error(y_test,y_pred)))\n",
    "        print(\"R-squared: {}\".format(r2_score(y_test,y_pred)))\n",
    "    except:\n",
    "        print(\"Sorry, error while execution.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Question 4:\n",
    "\n",
    "1. Derived variable `tip_percentage` has been build and used as a dependent variable.\n",
    "2. Sklearn pipeline has been created and perfomed all the data cleaning/transforming inside it.\n",
    "3. Pipeline predictive model is created using sklearn algorithms.\n",
    "4. Pipeline is also created in a way to use different algorithms and run CV.\n",
    "5. As the dependent variable is derived, the model provides higher weight to some of the independent variables.\n",
    "6. Better way to model is to remove tip and total amount from the dataset after calculating the tip. This can be easily perfomed in the above coding.\n",
    "7. Code has been completley modularized. It contains a config file where all the model parameters can be changed without changing the code.\n",
    "8. As the dependet variable is derived, model provides very good performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
